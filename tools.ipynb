{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Langchain Tool Bakeoff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import json\n",
    "import xml.etree.ElementTree as ET\n",
    "import os\n",
    "from collections import Counter\n",
    "from typing import Optional, Type\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "from langchain.tools import BaseTool\n",
    "from langchain.callbacks.manager import AsyncCallbackManagerForToolRun, CallbackManagerForToolRun\n",
    "\n",
    "from sumy.parsers.plaintext import PlaintextParser\n",
    "from sumy.nlp.tokenizers import Tokenizer\n",
    "from sumy.summarizers.lsa import LsaSummarizer as Summarizer\n",
    "from sumy.nlp.stemmers import Stemmer\n",
    "from sumy.utils import get_stop_words\n",
    "\n",
    "import crawler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "STOPWORDS = open('hit_stopwords.txt', 'r').read().split('\\n')\n",
    "def filenameParser(filename: str) -> tuple[str, str, str]:\n",
    "    filename = filename[:-4]\n",
    "    date, time, post_id = filename.split('_')\n",
    "    return date, time, post_id\n",
    "\n",
    "\n",
    "def findPostByID(post_id: str) -> str:\n",
    "    for year_dir in os.listdir(\"./HatePolitics/\"):\n",
    "        for filename in os.listdir(os.path.join(\"./HatePolitics/\", year_dir)):\n",
    "            if f\"{post_id}\" in filename:\n",
    "                return os.path.join(\"./HatePolitics/\", year_dir, filename)\n",
    "\n",
    "\n",
    "def getSummaryByContent(contents: list[str]) -> list[str]:\n",
    "    LANGUAGE = 'chinese'\n",
    "    stemmer = Stemmer(LANGUAGE)\n",
    "    summarizer = Summarizer(stemmer)\n",
    "    stop_words = set(get_stop_words(LANGUAGE))\n",
    "    summarizer.stop_words = stop_words.union(set(STOPWORDS))\n",
    "    tokenizer = Tokenizer(LANGUAGE)\n",
    "    summaries = []\n",
    "    for content in contents:\n",
    "        parser = PlaintextParser.from_string(\n",
    "            content, tokenizer,\n",
    "        )\n",
    "        summaries.append(\n",
    "            summarizer(parser.document, 1)[0]._text\n",
    "        )\n",
    "    return summaries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./HatePolitics/2023/20230105_1034_M.1672914887.A.04F.xml'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "findPostByID(\"M.1672914887.A.04F\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "'''\n",
    "Header format:\n",
    "<?xml version='1.0' encoding='utf-8'?>\n",
    "<TEI.2>\n",
    "   <teiHeader>\n",
    "      <metadata name=\"media\"></metadata>\n",
    "      <metadata name=\"author\"></metadata>\n",
    "      <metadata name=\"post_id\"></metadata>\n",
    "      <metadata name=\"year\"></metadata>\n",
    "      <metadata name=\"board\"></metadata>\n",
    "      <metadata name=\"title\"></metadata>\n",
    "   </teiHeader>\n",
    "   <text>\n",
    "      <body author=\"\"></body>\n",
    "      <title author=\"\"></title>\n",
    "      <comment author=\"\" c_type=\"\"></comment>\n",
    "      <!-- c_type {pos: 推, neu: 箭頭, neg: 噓 -->\n",
    "   </text>\n",
    "</TEI.2>\n",
    "'''\n",
    "directory = './HatePolitics/2023'\n",
    "files = os.listdir(directory)\n",
    "files.sort()\n",
    "\n",
    "start_date = 20230101\n",
    "end_date = 20230131\n",
    "in_range_post_ids = []\n",
    "for filename in files:\n",
    "    date = int(filename[:8])\n",
    "    if date >= start_date and date <= end_date:\n",
    "        in_range_post_ids.append(filename)\n",
    "\n",
    "# print(inrange_files)\n",
    "print(len(in_range_post_ids))\n",
    "\n",
    "# parse each xml file and get the author name\n",
    "author_count = Counter()\n",
    "min_score = 0\n",
    "max_score = 0\n",
    "min_author = ''\n",
    "max_author = ''\n",
    "min_title = ''\n",
    "max_title = ''\n",
    "\n",
    "for filename in in_range_post_ids:\n",
    "    tree = ET.parse(directory + '/' + filename)\n",
    "    root = tree.getroot()\n",
    "    author = root[0][1].text\n",
    "    title = root[0][5].text\n",
    "    author_count[author] += 1\n",
    "\n",
    "    # loop through all comments\n",
    "    score = 0\n",
    "    for comment in root[1][2:]:\n",
    "        print(comment)\n",
    "        print(comment.attrib)\n",
    "        comment_type = comment.attrib['c_type']\n",
    "        if comment_type == 'pos':\n",
    "            score += 1\n",
    "        elif comment_type == 'neg':\n",
    "            score -= 1\n",
    "\n",
    "    if score < min_score:\n",
    "        min_score = score\n",
    "        min_author = author\n",
    "        min_title = title\n",
    "    elif score > max_score:\n",
    "        max_score = score\n",
    "        max_author = author\n",
    "        max_title = title\n",
    "\n",
    "print(author_count.most_common(10))\n",
    "print(min_score, min_author, min_title)\n",
    "print(max_score, max_author, max_title)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = in_range_post_ids[0]\n",
    "tree = ET.parse(directory + '/' + filename)\n",
    "root = tree.getroot()\n",
    "print(root[1][2].attrib)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "XML file format:\n",
    "\n",
    "```xml\n",
    "<?xml version='1.0' encoding='utf-8'?>\n",
    "<TEI.2>\n",
    "   <teiHeader>\n",
    "      <metadata name=\"media\"></metadata>\n",
    "      <metadata name=\"author\"></metadata>\n",
    "      <metadata name=\"post_id\"></metadata>\n",
    "      <metadata name=\"year\"></metadata>\n",
    "      <metadata name=\"board\"></metadata>\n",
    "      <metadata name=\"title\"></metadata>\n",
    "   </teiHeader>\n",
    "   <text>\n",
    "      <body author=\"\">\n",
    "         <s>\n",
    "            <w type=\"\"></w> <!-- type=詞性標記 -->\n",
    "         </s>\n",
    "      </body>\n",
    "      <title author=\"\"></title>\n",
    "      <!-- c_type {pos: 推, neu: 箭頭, neg: 噓} -->\n",
    "      <comment author=\"\" c_type=\"\"></comment>\n",
    "   </text>\n",
    "</TEI.2>\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Reference](https://python.langchain.com/en/latest/modules/agents/tools/custom_tools.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Template for a tool\n",
    "class TemplateTool(BaseTool):\n",
    "    name = \"custom_search\"\n",
    "    description = \"useful for when you need to answer questions about current events\"\n",
    "\n",
    "    def _run(self,\n",
    "             query: str,\n",
    "             run_manager: Optional[CallbackManagerForToolRun] = None) -> str:\n",
    "        \"\"\"Use the tool.\"\"\"\n",
    "        return \"query\"\n",
    "    \n",
    "    async def _arun(self, query: str, run_manager: Optional[AsyncCallbackManagerForToolRun] = None) -> str:\n",
    "        \"\"\"Use the tool asynchronously.\"\"\"\n",
    "        raise NotImplementedError(\"custom_search does not support async\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GetPostsByDate(BaseTool):\n",
    "    \"\"\"\n",
    "    Get ptt posts in the database, by date\n",
    "    \"\"\"\n",
    "    name = \"get_posts_by_date\"\n",
    "    description = \"\"\"\n",
    "    Input: string of date, in format YYYYMMDD, (e.g. 20200101)\n",
    "    Output: list of post ids, serialized in json\n",
    "    \"\"\"\n",
    "\n",
    "    def _run(self,\n",
    "             query: str,\n",
    "             run_manager: Optional[CallbackManagerForToolRun] = None) -> str:\n",
    "        year, month, day = query[:4], query[4:6], query[6:]\n",
    "        assert len(year) == 4 and len(month) == 2 and len(day) == 2\n",
    "        directory = \"./HatePolitics/2023\"\n",
    "        in_range_post_ids = []\n",
    "        files = os.listdir(directory)\n",
    "        files.sort()\n",
    "        for filename in files:\n",
    "            date, time, post_id = filenameParser(filename)\n",
    "            if date == year + month + day:\n",
    "                in_range_post_ids.append(post_id)\n",
    "        return json.dumps(in_range_post_ids)\n",
    "\n",
    "    async def _arun(self,\n",
    "                    query: str,\n",
    "                    run_manager: Optional[CallbackManagerForToolRun] = None) -> str:\n",
    "        raise NotImplementedError(\"Doesn't support async\")\n",
    "\n",
    "\n",
    "class GetArrowCount(BaseTool):\n",
    "    \"\"\"\n",
    "    Get upvote count of a post\n",
    "    \"\"\"\n",
    "    name = \"get_upvote_count\"\n",
    "    description = \"\"\"\n",
    "    Input: post_id returned by get_posts_by_date (e.g. M.1672914887.A.04F)\n",
    "    Output: upvote count\n",
    "    \"\"\"\n",
    "\n",
    "    def _run(self,\n",
    "             query: str,\n",
    "             run_manager: Optional[CallbackManagerForToolRun] = None) -> str:\n",
    "        directory = \"./HatePolitics/2023\"\n",
    "        # filename = directory + '/' + query + '.xml'\n",
    "        filename = findPostByID(query)\n",
    "        tree = ET.parse(filename)\n",
    "        root = tree.getroot()\n",
    "        arrow_count = 0\n",
    "        for comment in root[1][2:]:\n",
    "            comment_type = comment.attrib['c_type']\n",
    "            if comment_type == 'neu':\n",
    "                arrow_count += 1\n",
    "        return str(arrow_count)\n",
    "\n",
    "    async def _arun(self,\n",
    "                    query: str,\n",
    "                    run_manager: Optional[CallbackManagerForToolRun] = None) -> str:\n",
    "        raise NotImplementedError(\"Doesn't support async\")\n",
    "\n",
    "\n",
    "class GetDownvoteCount(BaseTool):\n",
    "    \"\"\"\n",
    "    Get upvote count of a post\n",
    "    \"\"\"\n",
    "    name = \"get_upvote_count\"\n",
    "    description = \"\"\"\n",
    "    Input: post_id returned by get_posts_by_date (e.g. M.1672914887.A.04F)\n",
    "    Output: upvote count\n",
    "    \"\"\"\n",
    "\n",
    "    def _run(self,\n",
    "             query: str,\n",
    "             run_manager: Optional[CallbackManagerForToolRun] = None) -> str:\n",
    "        directory = \"./HatePolitics/2023\"\n",
    "        # filename = directory + '/' + query + '.xml'\n",
    "        filename = findPostByID(query)\n",
    "        tree = ET.parse(filename)\n",
    "        root = tree.getroot()\n",
    "        downvote_count = 0\n",
    "        for comment in root[1][2:]:\n",
    "            comment_type = comment.attrib['c_type']\n",
    "            if comment_type == 'neg':\n",
    "                downvote_count += 1\n",
    "        return str(downvote_count)\n",
    "\n",
    "    async def _arun(self,\n",
    "                    query: str,\n",
    "                    run_manager: Optional[CallbackManagerForToolRun] = None) -> str:\n",
    "        raise NotImplementedError(\"Doesn't support async\")\n",
    "\n",
    "\n",
    "class GetUpvoteCount(BaseTool):\n",
    "    \"\"\"\n",
    "    Get upvote count of a post\n",
    "    \"\"\"\n",
    "    name = \"get_upvote_count\"\n",
    "    description = \"\"\"\n",
    "    Input: post_id returned by get_posts_by_date (e.g. M.1672914887.A.04F)\n",
    "    Output: upvote count\n",
    "    \"\"\"\n",
    "\n",
    "    def _run(self,\n",
    "             query: str,\n",
    "             run_manager: Optional[CallbackManagerForToolRun] = None) -> str:\n",
    "        directory = \"./HatePolitics/2023\"\n",
    "        # filename = directory + '/' + query + '.xml'\n",
    "        filename = findPostByID(query)\n",
    "        tree = ET.parse(filename)\n",
    "        root = tree.getroot()\n",
    "        upvote_count = 0\n",
    "        for comment in root[1][2:]:\n",
    "            comment_type = comment.attrib['c_type']\n",
    "            if comment_type == 'pos':\n",
    "                upvote_count += 1\n",
    "        return str(upvote_count)\n",
    "\n",
    "    async def _arun(self,\n",
    "                    query: str,\n",
    "                    run_manager: Optional[CallbackManagerForToolRun] = None) -> str:\n",
    "        raise NotImplementedError(\"Doesn't support async\")\n",
    "\n",
    "\n",
    "class GetPostSummary(BaseTool):\n",
    "    \"\"\"\n",
    "    Get summary of a post by post_id\n",
    "    \"\"\"\n",
    "    name = \"get_post_summary\"\n",
    "    description = \"\"\"\n",
    "    Input: post_id returned by get_posts_by_date (e.g. M.1672914887.A.04F)\n",
    "    Output: post summary\n",
    "    \"\"\"\n",
    "\n",
    "    def _run(self,\n",
    "             query: str,\n",
    "             run_manager: Optional[CallbackManagerForToolRun] = None) -> str:\n",
    "        filename = findPostByID(query)\n",
    "        tree = ET.parse(filename)\n",
    "        root = tree.getroot()\n",
    "        body_node = root.find(\"body\")\n",
    "        body_text = [\n",
    "            \"\".join([w.text for w in s.findall(\"w\")]) for s in body_node.findall(\"s\")\n",
    "        ]\n",
    "        summary = getSummaryByContent(body_text)\n",
    "        return summary\n",
    "\n",
    "    async def _arun(self,\n",
    "                    query: str,\n",
    "                    run_manager: Optional[CallbackManagerForToolRun] = None) -> str:\n",
    "        raise NotImplementedError(\"Doesn't support async\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class GetPostsTitlesByCrawler(BaseTool):\n",
    "    \"\"\"\n",
    "    Get latest news posts titles from crawler\n",
    "    Support website: \n",
    "        (default): https://news.pts.org.tw/category/1\n",
    "        https://news.ttv.com.tw/category/%E6%94%BF%E6%B2%BB\n",
    "    \"\"\"\n",
    "    name = \"get_posts_titles_by_crawler\"\n",
    "    description = \"獲得近期政治新聞標題\"\n",
    "\n",
    "    def _run(\n",
    "        self,\n",
    "        query: str,\n",
    "        run_manager: Optional[CallbackManagerForToolRun] = None\n",
    "    ) -> str:\n",
    "        posts = crawler.politic_news_crawler('pts', cnt=100)\n",
    "        titles = [post['title'] for post in posts]\n",
    "        return json.dumps(titles)\n",
    "\n",
    "    async def _arun(\n",
    "        self,\n",
    "        query: str,\n",
    "        run_manager: Optional[CallbackManagerForToolRun] = None\n",
    "    ) -> str:\n",
    "        raise NotImplementedError(\"Doesn't support async\")\n",
    "\n",
    "\n",
    "class GetPostsSummaryByCrawler(BaseTool):\n",
    "    \"\"\"\n",
    "    Get latest news posts summary content from crawler\n",
    "    Summarize by package sumy using LsaSummarizer\n",
    "    Support website: \n",
    "        (default): https://news.pts.org.tw/category/1\n",
    "        https://news.ttv.com.tw/category/%E6%94%BF%E6%B2%BB\n",
    "    \"\"\"\n",
    "    name = \"get_posts_titles_by_crawler\"\n",
    "    description = \"獲得近期政治新聞內文概述\"\n",
    "    LANGUAGE = \"chinese\"\n",
    "    tokenizer = Tokenizer(LANGUAGE)\n",
    "\n",
    "    def summarize(self, contents):\n",
    "        parser = PlaintextParser.from_string(\n",
    "            contents,\n",
    "            self.tokenizer,\n",
    "        )\n",
    "        summaries = summarizer\n",
    "        return summaries\n",
    "\n",
    "    def _run(\n",
    "        self,\n",
    "        query: str,\n",
    "        run_manager: Optional[CallbackManagerForToolRun] = None\n",
    "    ) -> str:\n",
    "        posts = crawler.politic_news_crawler('pts', cnt=100)\n",
    "        contents = [post['content'] for post in posts]\n",
    "        summaries = summaries(contents)\n",
    "        return json.dumps(summaries)\n",
    "\n",
    "    async def _arun(\n",
    "        self,\n",
    "        query: str,\n",
    "        run_manager: Optional[CallbackManagerForToolRun] = None\n",
    "    ) -> str:\n",
    "        raise NotImplementedError(\"Doesn't support async\")\n",
    "\n",
    "\n",
    "class GetPostsKeywordsByCrawler(BaseTool):\n",
    "    \"\"\"\n",
    "    Support website: \n",
    "        (default): https://news.pts.org.tw/category/1\n",
    "        https://news.ttv.com.tw/category/%E6%94%BF%E6%B2%BB\n",
    "    \"\"\"\n",
    "    name = \"get_posts_titles_by_crawler\"\n",
    "    description = \"獲得近期政治新聞內文關鍵字\"\n",
    "    LANGUAGE = \"chinese\"\n",
    "    tokenizer = Tokenizer(LANGUAGE)\n",
    "\n",
    "    def _run(\n",
    "        self,\n",
    "        query: str,\n",
    "        run_manager: Optional[CallbackManagerForToolRun] = None\n",
    "    ) -> str:\n",
    "        posts = crawler.politic_news_crawler('pts', cnt=100)\n",
    "        contents = [post['content'] for post in posts]\n",
    "        summaries = summaries(contents)\n",
    "        return json.dumps(summaries)\n",
    "\n",
    "    async def _arun(\n",
    "        self,\n",
    "        query: str,\n",
    "        run_manager: Optional[CallbackManagerForToolRun] = None\n",
    "    ) -> str:\n",
    "        raise NotImplementedError(\"Doesn't support async\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "dataset =  [\n",
    "     '''\n",
    "     Extremely Severe Cyclonic Storm Mocha was a powerful and deadly tropical cyclone in the North Indian Ocean which affected Myanmar and parts of Bangladesh in May 2023. The second depression and the first cyclonic storm of the 2023 North Indian Ocean cyclone season, Mocha originated from a low-pressure area that was first noted by the India Meteorological Department (IMD) on 8 May. After consolidating into a depression, the storm tracked slowly north-northwestward over the Bay of Bengal, and reached extremely severe cyclonic storm intensity. After undergoing an eyewall replacement cycle, Mocha rapidly strengthened, peaking at Category 5-equivalent intensity on 14 May with winds of 280 km/h (175 mph), tying with Cyclone Fani as the strongest storm on record in the north Indian Ocean, in terms of 1-minute sustained winds. Mocha slightly weakened before making landfall, and its conditions quickly became unfavorable. Mocha rapidly weakened once inland and dissipated shortly thereafter.\n",
    "Thousands of volunteers assisted citizens of Myanmar and Bangladesh in evacuating as the cyclone approached the international border.[6] Evacuations were also ordered for low-lying areas in Sittwe, Pauktaw, Myebon, Maungdaw, and Buthidaung. In Bangladesh, over 500,000 individuals were ordered to be relocated to coastal areas of the country due to the storm's approach. Officials from the military declared the state of Rakhine a natural disaster area. Several villages in Rakhine State were also damaged by the cyclone.\n",
    "Cyclone Mocha killed at least 463 people, including three indirect deaths in Bangladesh. It also injured 719 people, and left 101 others missing.[7][5] The storm caused about US$1.07 million of damage in Bangladesh.[8] '''\n",
    "     'Kumquat plants have thornless branches and extremely glossy leaves. They bear dainty white flowers that occur in clusters or individually inside the leaf axils. The plants can reach a height from 2.5 to 4.5 metres (8.2 to 14.8 ft), with dense branches, sometimes bearing small thorns.[5] They bear yellowish-orange fruits that are oval or round in shape. The fruits can be 1 inch (2.5 cm) in diameter and have a sweet, pulpy skin and slightly acidic inner pulp. All the kumquat trees are self-pollinating. Kumquats can tolerate both frigid and hot temperatures',\n",
    "     '''The photo portrays fourteen Israeli soldiers in an abandoned barracks with traditional army dinnerware. Unlike the original painting, Nes' version lacks tension and shows the soldiers in private conversations, while the central figure (Jesus) \"stares vacantly into space\". The artist does not provide a specific interpretation, but expresses sympathy and hope that it is not their last meal together. One extra person is added to avoid \"direct quotation\" of Leonardo da Vinci.[7] The fourteenth man (standing at the left) is the only one, apart for the central figure, who is not engaged in a conversations and looks apart, and the only one whose uniform shows the Israeli Defense Forces patch''',\n",
    "     'Is this the first document?',\n",
    " ]\n",
    "tfIdfVectorizer=TfidfVectorizer(use_idf=True, stop_words=['the', 'in'])\n",
    "tfIdf = tfIdfVectorizer.fit_transform(dataset)\n",
    "df = pd.DataFrame(tfIdf[0].T.todense(), index=tfIdfVectorizer.get_feature_names_out(), columns=[\"TF-IDF\"])\n",
    "df = df.sort_values('TF-IDF', ascending=False)\n",
    "print (df.head(25))\n",
    "\n",
    "\n",
    "class GetTfidfKeywords(BaseTool):\n",
    "    name = \"Tfidf\"\n",
    "    description = \"try to extract keywords by tfidf\"\n",
    "\n",
    "    def _run(self,\n",
    "             query: str,\n",
    "             run_manager: Optional[CallbackManagerForToolRun] = None) -> str:\n",
    "        \"\"\"Use the tool.\"\"\"\n",
    "        dataset = eval(query)\n",
    "        return \"query\"\n",
    "    \n",
    "    async def _arun(self, query: str, run_manager: Optional[AsyncCallbackManagerForToolRun] = None) -> str:\n",
    "        \"\"\"Use the tool asynchronously.\"\"\"\n",
    "        raise NotImplementedError(\"custom_search does not support async\")\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cllt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
